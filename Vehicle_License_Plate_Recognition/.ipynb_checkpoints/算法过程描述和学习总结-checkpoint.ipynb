{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据分布：\n",
    "> 遍历文件夹，选取其中文件数目最少的文件夹文件数目（203张），每个标签选取此文件夹数目的75%作为训练集（152张），25%作为验证集（51张），总训练集图片有5168张，测试集有1734张\n",
    "![训练集](../images/train.png)![测试集](../images/test.png)  \n",
    "\n",
    "### 网络结构：\n",
    "> \n",
    "- 隐藏层一共有三层卷积层、三层池化层和一层全连接层\n",
    "- 卷积核大小为3*3\n",
    "- 激活函数使用了relu激活函数\n",
    "- 优化条件有\n",
    "   1. dropout正则化   \n",
    "   2. 数据处理时每个标签取出数目公平，后期为保证随机性，使用了shufflebatch随机出队，并  \n",
    " 在此基础上再次打乱以保证其随机性       \n",
    "   3. 使用了Adam优化函数 \n",
    "   4. 通过计算验证集每代的损失，保存了损失最小的那代模型\n",
    "   \n",
    "### 测试：\n",
    "> 正确率：   0.916783   \n",
    "  召回率：\n",
    "  - 标签 0 的召回率为 0.949580\n",
    "  - 标签 1 的召回率为 0.925926\n",
    "  - 标签 2 的召回率为 0.993289\n",
    "  - 标签 3 的召回率为 0.992366\n",
    "  - 标签 4 的召回率为 0.983471\n",
    "  - 标签 5 的召回率为 0.985915\n",
    "  - 标签 6 的召回率为 1.000000\n",
    "  - 标签 7 的召回率为 0.992188\n",
    "  - 标签 8 的召回率为 0.993243\n",
    "  - 标签 9 的召回率为 0.972973\n",
    "  - 标签 A 的召回率为 0.885135\n",
    "  - 标签 B 的召回率为 0.810000\n",
    "  - 标签 C 的召回率为 0.862500\n",
    "  - 标签 D 的召回率为 0.808824\n",
    "  - 标签 E 的召回率为 0.933333\n",
    "  - 标签 F 的召回率为 0.800000\n",
    "  - 标签 G 的召回率为 0.362319\n",
    "  - 标签 H 的召回率为 0.960000\n",
    "  - 标签 J 的召回率为 0.886076\n",
    "  - 标签 K 的召回率为 0.826087\n",
    "  - 标签 L 的召回率为 0.975000\n",
    "  - 标签 M 的召回率为 1.000000\n",
    "  - 标签 N 的召回率为 0.632353\n",
    "  - 标签 P 的召回率为 0.938462\n",
    "  - 标签 Q 的召回率为 0.983607\n",
    "  - 标签 R 的召回率为 0.898551\n",
    "  - 标签 S 的召回率为 0.745455\n",
    "  - 标签 T 的召回率为 0.928571\n",
    "  - 标签 U 的召回率为 0.528571\n",
    "  - 标签 V 的召回率为 0.652174\n",
    "  - 标签 W 的召回率为 0.975000\n",
    "  - 标签 X 的召回率为 0.995943\n",
    "  - 标签 Y 的召回率为 0.970000\n",
    "  - 标签 Z 的召回率为 0.971429"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习总结：\n",
    "> 在本次完成大作业过程中，经过探讨、询问、资料查询，得到很多收获。\n",
    "- 通过实践更深刻的理解了卷积与池化的工作原理，明白了卷积核大小如何选取及其与卷积层数的关系\n",
    "- 知道如何使用relu激活函数避免梯度消失问题，了解了relu函数的延伸prelu函数和lrelu函数\n",
    "- 知道通过adam优化使神经网络实现了反馈\n",
    "- 明白了如何通过验证集防止模型被训练得过拟合，懂得了图片处理的一些方式，tfrecord的存取、Image的resize方法([W,H])、tf的reshape方法([H,W,C])、转灰度图像、归一化处理、shuffle_batch随机出队、图像色彩空间变换和利用图表展示等等\n",
    "- 懂得了正则化方法dropout的原理，重复多次将一部分特征停用，由一个神经网络得到无数个子网络，投票选举获得更好的模型，dropout最适用于隐藏层最后一层\n",
    "- 对模型存储有了更深入的了解\n",
    "- 还知道了由于jupyter自身会将重复的变量名做更改以方便辨认，读取训练模型时不要重复加载定义模型的代码，今后代码将尽量多以功能分块，不聚集在一个代码块中。    \n",
    "\n",
    "> 结果总结：观察了召回率低的标签样本，初步推测是由于测试集部分样本稍微倾斜，数据预处理不完善导致的识别出错，考虑下次完善预处理部分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[打印原标签和预测标签的文件链接](test_labels.txt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
