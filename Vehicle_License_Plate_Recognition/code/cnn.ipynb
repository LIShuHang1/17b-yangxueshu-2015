{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from read_data import dataset, dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义存储地址与名称\n",
    "CNN = '/home/aa/Vehicle_License_Plate_Recognition/data/cnn'\n",
    "cnn = 'License_plate'\n",
    "#定义模型文件所在的文件夹，若不存在则自动创建\n",
    "ckpt = tf.train.latest_checkpoint(CNN)\n",
    "if not ckpt:\n",
    "    if not os.path.exists(CNN):\n",
    "        os.mkdir(CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 卷积神经网络（有正则化的添加与使用）\n",
    "def weight_variable(shape):  \n",
    "    # 使用截断正态分布生成卷积核\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)  \n",
    "    return tf.Variable(initial)  \n",
    "  \n",
    "def bias_variable(shape):  \n",
    "    # 使用relu激活函数，用一个正偏置值较准\n",
    "    initial = tf.constant(0.1, shape=shape)  \n",
    "    return tf.Variable(initial)  \n",
    "  \n",
    "def conv2d(x, W):  \n",
    "    #定义卷积层\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')  \n",
    "  \n",
    "def max_pool_2x2(x):  \n",
    "    # 定义池化层\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')  \n",
    "\n",
    "\n",
    "# min_next_batch_tfr(随机批次载入数据)\n",
    "def min_next_batch_tfr(image, label, num=50, num1=500): \n",
    "    images = np.zeros((num, 1152))\n",
    "    labels = np.zeros((num, 34))\n",
    "    for i in range(num):\n",
    "        temp = random.randint(0, num1-1)\n",
    "        images[i, :] = image[temp]\n",
    "        labels[i, :] = label[temp]\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 1152])\n",
    "y_ = tf.placeholder(tf.float32, [None, 34])\n",
    "keep_prob = tf.placeholder(\"float\")  \n",
    "\n",
    "# 第一层 先卷积再池化，卷积核尺寸为3*3, 通道数为1，输出通道为32，池化输出后应为24*12*32\n",
    "W_conv1 = weight_variable([3, 3, 1, 32])  \n",
    "b_conv1 = bias_variable([32])  \n",
    "# 格式转换\n",
    "x_image = tf.reshape(x, [-1, 48, 24, 1])  \n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)  \n",
    "h_pool1 = max_pool_2x2(h_conv1)  \n",
    "\n",
    "# 第二层 先卷积再池化，卷积核尺寸为3*3, 输入通道为32，输出通道为64，池化输出后应为12*6*64\n",
    "W_conv2 = weight_variable([3, 3, 32, 64])  \n",
    "b_conv2 = bias_variable([64])  \n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)  \n",
    "h_pool2 = max_pool_2x2(h_conv2)  \n",
    "  \n",
    "# 第三层 先卷积再池化，卷积核尺寸为3*3, 输入通道为64，输出通道为96，池化输出后应为6*3*96\n",
    "W_conv3 = weight_variable([3, 3, 64, 96])  \n",
    "b_conv3 = bias_variable([96])  \n",
    "h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)  \n",
    "h_pool3 = max_pool_2x2(h_conv3)  \n",
    "\n",
    "# 此时图片格式简化为6*3，进入第四层全连接层,输入维数6*3*96, 输出维数为1024\n",
    "W_fc1 = weight_variable([6 * 3 * 96, 1024])  \n",
    "b_fc1 = bias_variable([1024])  \n",
    "h_pool2_flat = tf.reshape(h_pool3, [-1, 6*3*96])  \n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)  \n",
    "# 使用drop out防止过拟合（正则化）\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)  \n",
    "\n",
    "# 第五层，输入1024维，输出34维，即0~33分类\n",
    "W_fc2 = weight_variable([1024, 34])  \n",
    "b_fc2 = bias_variable([34])   \n",
    "y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)  \n",
    "saver=tf.train.Saver(max_to_keep=1)\n",
    "\n",
    "# 损失函数，交叉熵 \n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))  \n",
    "# 使用adam优化\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy) \n",
    "# 准确度计算\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))  \n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "done\n",
      "training...\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = dataset2('train', 5168)\n",
    "x_test, y_test = dataset2('test', 1734)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, validating accuracy 0.0334487, loss is 12608.7\n",
      "step 100, validating accuracy 0.328143, loss is 5025.43\n",
      "step 200, validating accuracy 0.696655, loss is 3540.35\n",
      "step 300, validating accuracy 0.859285, loss is 2186.88\n",
      "step 400, validating accuracy 0.89504, loss is 1385.99\n",
      "step 500, validating accuracy 0.915802, loss is 968.809\n",
      "step 600, validating accuracy 0.9406, loss is 686.146\n",
      "step 700, validating accuracy 0.94579, loss is 545.651\n",
      "step 800, validating accuracy 0.963668, loss is 428.074\n",
      "step 900, validating accuracy 0.970588, loss is 330.298\n",
      "step 1000, validating accuracy 0.975779, loss is 292.882\n",
      "step 1100, validating accuracy 0.978662, loss is 247.698\n",
      "step 1200, validating accuracy 0.980969, loss is 225.71\n",
      "step 1300, validating accuracy 0.982699, loss is 208.798\n",
      "step 1400, validating accuracy 0.983276, loss is 181.239\n",
      "step 1500, validating accuracy 0.986159, loss is 169.094\n",
      "step 1600, validating accuracy 0.987313, loss is 155.553\n",
      "step 1700, validating accuracy 0.989619, loss is 145.013\n",
      "step 1800, validating accuracy 0.985006, loss is 150.253\n",
      "step 1900, validating accuracy 0.984429, loss is 148.748\n",
      "step 2000, validating accuracy 0.990196, loss is 128.179\n",
      "step 2100, validating accuracy 0.988466, loss is 126.674\n",
      "step 2200, validating accuracy 0.986736, loss is 135.133\n",
      "step 2300, validating accuracy 0.990196, loss is 115.606\n",
      "step 2400, validating accuracy 0.987889, loss is 126.479\n",
      "step 2500, validating accuracy 0.989043, loss is 111.538\n",
      "step 2600, validating accuracy 0.986736, loss is 122.447\n",
      "step 2700, validating accuracy 0.990773, loss is 109.11\n",
      "step 2800, validating accuracy 0.989043, loss is 109.442\n",
      "step 2900, validating accuracy 0.990773, loss is 109.159\n",
      "step 3000, validating accuracy 0.990196, loss is 109.298\n",
      "[12608.669, 5025.4336, 3540.3521, 2186.8774, 1385.9905, 968.80865, 686.14575, 545.65063, 428.07404, 330.29785, 292.88165, 247.69794, 225.71036, 208.79813, 181.23947, 169.09431, 155.5528, 145.01343, 150.25262, 148.74847, 128.17896, 126.67428, 135.13278, 115.60571, 126.47938, 111.53828, 122.44663, 109.10954, 109.44247, 109.15927, 109.29759]\n",
      "109.11\n"
     ]
    }
   ],
   "source": [
    "# 训练模型，训练过程中每一百次插播验证集当前识别率\n",
    "with tf.Session() as sess: \n",
    "    # 运行会话\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # 变量定义\n",
    "    step = 0\n",
    "    j = 0 # 有三代loss都比minloss大则保存模型\n",
    "    losslist = []\n",
    "    minloss = 100000\n",
    "    # 若模型存在，自动加载模型进会话\n",
    "    ckpt = tf.train.latest_checkpoint(CNN)\n",
    "    if ckpt:\n",
    "        saver.restore(sess=sess,save_path=ckpt)\n",
    "        step = int(ckpt[len(os.path.join(CNN, cnn)) + 1:])\n",
    "    ckptname=os.path.join(CNN, cnn)\n",
    "    \n",
    "    # 开启线程\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    \n",
    "    # 训练\n",
    "    for i in range(4000):\n",
    "        batch = min_next_batch_tfr(x_train, y_train, 50, 5168)\n",
    "        train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "        # 每一百次用验证集测试一次\n",
    "        if i % 100 == 0:\n",
    "            train_accuracy, loss = sess.run([accuracy, cross_entropy], feed_dict={\n",
    "                x: x_test, y_: y_test, keep_prob: 1})\n",
    "            print(\"step %d, validating accuracy %g, loss is %g\" % (i, train_accuracy, loss))\n",
    "            losslist.append(loss)\n",
    "        # 保存损失最低的模型\n",
    "        if minloss > loss:\n",
    "            minloss = loss\n",
    "            saver.save(sess,ckptname,global_step=i)\n",
    "        if losslist[-1] > minloss and losslist[-2] > minloss and losslist[-3] > minloss:\n",
    "            break\n",
    "    \n",
    "    # 损失函数列表\n",
    "    print(losslist)\n",
    "    print(minloss)\n",
    "    coord.request_stop() \n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/aa/Vehicle_License_Plate_Recognition/data/cnn/License_plate-2700\n",
      "test accuracy 0.990773\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "标签 0 的召回率为 0.980392\n",
      "标签 1 的召回率为 1.000000\n",
      "标签 2 的召回率为 0.960784\n",
      "标签 3 的召回率为 1.000000\n",
      "标签 4 的召回率为 0.980392\n",
      "标签 5 的召回率为 1.000000\n",
      "标签 6 的召回率为 0.980392\n",
      "标签 7 的召回率为 1.000000\n",
      "标签 8 的召回率为 0.980392\n",
      "标签 9 的召回率为 1.000000\n",
      "标签 A 的召回率为 1.000000\n",
      "标签 B 的召回率为 1.000000\n",
      "标签 C 的召回率为 1.000000\n",
      "标签 D 的召回率为 0.960784\n",
      "标签 E 的召回率为 1.000000\n",
      "标签 F 的召回率为 0.980392\n",
      "标签 G 的召回率为 0.980392\n",
      "标签 H 的召回率为 1.000000\n",
      "标签 J 的召回率为 0.980392\n",
      "标签 K 的召回率为 1.000000\n",
      "标签 L 的召回率为 1.000000\n",
      "标签 M 的召回率为 0.980392\n",
      "标签 N 的召回率为 1.000000\n",
      "标签 P 的召回率为 1.000000\n",
      "标签 Q 的召回率为 0.980392\n",
      "标签 R 的召回率为 1.000000\n",
      "标签 S 的召回率为 0.960784\n",
      "标签 T 的召回率为 0.980392\n",
      "标签 U 的召回率为 1.000000\n",
      "标签 V 的召回率为 1.000000\n",
      "标签 W 的召回率为 1.000000\n",
      "标签 X 的召回率为 1.000000\n",
      "标签 Y 的召回率为 1.000000\n",
      "标签 Z 的召回率为 1.000000\n"
     ]
    }
   ],
   "source": [
    "# 验证数据集判断模型效果\n",
    "with tf.Session() as sess: \n",
    "    # 运行会话\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    step=0\n",
    "    # 若模型存在，自动加载模型进会话\n",
    "    ckpt = tf.train.latest_checkpoint(CNN)\n",
    "    if ckpt:\n",
    "        saver.restore(sess=sess,save_path=ckpt)\n",
    "        step = int(ckpt[len(os.path.join(CNN, cnn)) + 1:])\n",
    "            \n",
    "    # 开启线程\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    \n",
    "    # 测试\n",
    "    print(\"test accuracy %g\" % accuracy.eval(feed_dict={\n",
    "        x: x_test, y_: y_test, keep_prob: 1.0}))\n",
    "    L_lmax = {}.fromkeys(range(34), 0)\n",
    "    L_lsame = {}.fromkeys(range(34), 0)\n",
    "    CL = {0:'0',1:'1',2:'2',3:'3',4:'4',5:'5',6:'6',7:'7',8:'8',9:'9',\n",
    "     10:'A',11:'B',12:'C',13:'D',14:'E',15:'F',16:'G',17:'H',18:'J',19:'K',\n",
    "     20:'L',21:'M',22:'N',23:'P',24:'Q',25:'R',26:'S',27:'T',28:'U',29:'V',\n",
    "     30:'W',31:'X',32:'Y',33:'Z'}\n",
    "    prediction_label = tf.argmax(y_conv.eval(feed_dict={\n",
    "        x: x_test, keep_prob: 1.0}), 1)\n",
    "    for i in range(len(x_test)):\n",
    "        if i%100 ==0:\n",
    "            print(i)\n",
    "        L_lmax[int(np.argmax(y_test[i]))] += 1\n",
    "        if np.argmax(y_test[i]) == prediction_label[i].eval():\n",
    "            L_lsame[int(np.argmax(y_test[i]))] += 1\n",
    "    for i in range(34):\n",
    "        recall_rate = L_lsame[i]/L_lmax[i]\n",
    "        print(\"标签 %s 的召回率为 %f\" %(CL[i],recall_rate))\n",
    "        \n",
    "    coord.request_stop() \n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
